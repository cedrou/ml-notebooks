{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic-Tac-Toe\n",
    "\n",
    "> Tic-tac-toe is a paper-and-pencil game for two players, X and O, who take turns marking the spaces in a 3Ã—3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game.\n",
    "> [Wikipedia](https://en.wikipedia.org/wiki/Tic-tac-toe)\n",
    "\n",
    "At a high level, we will need two data structures: \n",
    "\n",
    "- the __environment__ that manages the current state of the game (the grid)\n",
    "- the __agent__ that learns the optimal way to play the game \n",
    "\n",
    "During an episode, there will be 2 instances of the agent class and they both interact with the same instance of the environment. the `play_game()` function will run the main loop of the game.\n",
    "\n",
    "The grid is represented by a 3x3 `numpy.array`. The 3 different states of a cell are represented by 3 integers: 0 for empty, -1 and 1 for O and X. We choose these opposite values to help finding out an alignment by just summing the rows, columns and diagonals of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "Debug = 0\n",
    "BoardWidth = 3\n",
    "BoardHeight = 3\n",
    "X = 1\n",
    "O = -1\n",
    "_ = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `find_winner` analyzes the provided grid and returns:\n",
    "\n",
    "- `X` or `O` if the agent has won\n",
    "- `_` if there is a draw\n",
    "- `None` if there is no winner and it is not a terminal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_winner(board):\n",
    "    \n",
    "    # return O or X if there is 3 aligned symbols\n",
    "    for i in range(3):\n",
    "        s = np.sum(board[i,:]) \n",
    "        if s == 3*X: return X\n",
    "        if s == 3*O: return O\n",
    "\n",
    "    for i in range(3):\n",
    "        s = np.sum(board[:,i]) \n",
    "        if s == 3*X: return X\n",
    "        if s == 3*O: return O\n",
    "        \n",
    "    s = np.trace(board)\n",
    "    if s == 3*X: return X\n",
    "    if s == 3*O: return O\n",
    "\n",
    "    s = np.trace(np.fliplr(board))\n",
    "    if s == 3*X: return X\n",
    "    if s == 3*O: return O\n",
    "\n",
    "    # return None if the board is not full (at least one cell is empty)\n",
    "    for row in board:\n",
    "        for v in row:\n",
    "            if v == _: return None\n",
    "        \n",
    "    # the board is full, so this is a draw\n",
    "    return _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game has a finite number of states $N = 3^{rows \\times cols} = 3^9 = 19683$. This number is relatively small so we can enumerate all the possible states and decide for each if it is a terminal state with a winner or a draw. \n",
    "\n",
    "But first, we need a way to identify each different state. The function `compute_state_hash` computes a hash value that takes into account the state of each cell of the board. We can see the grid as a succession of cells that can take 3 different values. This is just like a number written with digits in base 3. \n",
    "\n",
    "$$\\{ B_{0,0}, B_{0,1}, \\ldots , B_{2,1}, B_{2,2} \\}$$\n",
    "\n",
    "$$H = 3^8*B_{0,0} + 3^7 * B_{0,1} + \\ldots + 3^1 * B_{2,1} + 3^0 * B_{2,2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_hash(board):\n",
    "    #   hash = B(2,2) + 3*(B(2,1) + 3*(B(2,0) + 3*(B(1,2) ... + B(0,0))))\n",
    "    v = board.reshape(-1)\n",
    "    p = np.power(3, np.arange(len(v)))\n",
    "    return int(np.sum(v * p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`enumerate_states_and_winner` is a recursive function that will go through all the possible states of the grid. For each state, it will precalculate if it is a terminal state (one players wins or the game is a draw), or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def enumerate_states_and_winner(board = None, x = 0, y = 0):\n",
    "    \n",
    "    states = {}\n",
    "    \n",
    "    if board is None:\n",
    "        board = np.empty((3,3))\n",
    "        \n",
    "    for symbol in (_, X, O):\n",
    "        board[x,y] = symbol\n",
    "        \n",
    "        if y == 2:\n",
    "            if x == 2:\n",
    "                s = compute_state_hash(board)\n",
    "                w = find_winner(board)\n",
    "                states[s] = w\n",
    "            \n",
    "            else:\n",
    "                states = {**states, **enumerate_states_and_winner(board, x+1, 0)}\n",
    "        \n",
    "        else:\n",
    "            states = {**states, **enumerate_states_and_winner(board, x, y+1)}\n",
    "    \n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Agent` class represent one player that can interact with the game and that will learn how to win.\n",
    "\n",
    "The `value` property is a data structure that maps a number to each possible state of the grid. This number gives a hint about the future goodness of a state and takes into account the probability of all possible future rewards. The estimation of this function is the key task in this problem as the agent will take its decision based on the value of each state it can choose.\n",
    "\n",
    "At the beginning, the only states we know the value are the terminal states, those for which the agent wins ($V(s) = 1$) and those for which it loses or nobody wins ($V(s) = 0$). All other states will be initialized with the value 0.5 because we don't know the future of this state.\n",
    "\n",
    "After each episode, the agent will update this function by 'back-propagating' the final state value into the previous ones with a learning rate $\\alpha$: $V(s) = V(s) + \\alpha (V(s+1) - V(s))$. To allow this, the agent needs to keep an history of each state it reaches during the game.\n",
    "\n",
    "At each turn, the agent will have the choice to explore a random strategy, or to exploit what he has learnt so far. For this, we will use the epsilon-greedy strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, symbol, env, alpha, epsilon):\n",
    "        self.symbol = symbol\n",
    "        self.state_history = []\n",
    "        self.value = {s: 1 if w==symbol else 0.5 if w is None else 0 for s,w in env.states.items()}\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    \n",
    "    def play(self, env):\n",
    "        \n",
    "        # find empty cells\n",
    "        empty_cells = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if env.board[i,j] == _:\n",
    "                    empty_cells.append((i,j))\n",
    "        \n",
    "        # take the decision to explore or to exploit\n",
    "        p = np.random.random()\n",
    "        if p < self.epsilon:\n",
    "            # --- exploration ---\n",
    "            # choose a random empty cell\n",
    "            ij = empty_cells[np.random.choice(len(empty_cells))]\n",
    "            env.board[ij] = self.symbol\n",
    "            if Debug:\n",
    "                print(\"Agent is playing random in\", ij)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            # --- exploitation ---\n",
    "            # choose the cell that have the highest value\n",
    "            maxV = -1\n",
    "            maxIJ = None\n",
    "            \n",
    "            # display cells values\n",
    "            if Debug:\n",
    "                for i in range(3):\n",
    "                    for j in range(3):\n",
    "                        if env.board[i,j] == _:\n",
    "                            env.board[i,j] = self.symbol\n",
    "                            v = self.value[compute_state_hash(env.board)]\n",
    "                            env.board[i,j] = _\n",
    "                            print(f'{v:0.2f} ', end='')\n",
    "                        else:\n",
    "                            print('  X  ' if env.board[i,j]==X else '  O  ', end='')\n",
    "                    print()\n",
    "                \n",
    "\n",
    "            for ij in empty_cells:\n",
    "                env.board[ij] = self.symbol\n",
    "                v = self.value[compute_state_hash(env.board)]\n",
    "                env.board[ij] = _\n",
    "                if v > maxV:\n",
    "                    maxV = v\n",
    "                    maxIJ = ij\n",
    "\n",
    "            env.board[maxIJ] = self.symbol\n",
    "            if Debug:\n",
    "                print(\"Agent is playing greedy in\", maxIJ)\n",
    "    \n",
    "    \n",
    "    def update(self, env):\n",
    "        self.state_history.reverse()\n",
    "        \n",
    "        # back-propagation of the final state value\n",
    "        next_state = self.state_history[0]\n",
    "        for s in self.state_history[1:]:\n",
    "            self.value[s] = self.value[s] + self.alpha * (self.value[next_state] - self.value[s])\n",
    "            next_state = s\n",
    "        \n",
    "        # clear the history for next episode\n",
    "        self.state_history = []\n",
    "        \n",
    "    \n",
    "    def update_state_history(self, state):\n",
    "        self.state_history.append(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Environment` class owns the board and can tell for each state if there is a winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, w, h):\n",
    "        self.board = None\n",
    "        self.states = enumerate_states_and_winner()\n",
    "    \n",
    "    def new_game(self):\n",
    "        self.board = np.ones((3,3)) * _\n",
    "        \n",
    "    def game_over(self):\n",
    "        h = compute_state_hash(self.board)\n",
    "        if self.states[h] is None: return False\n",
    "        \n",
    "        if Debug:\n",
    "            if self.states[h] == X:\n",
    "                print(\"  ###  X won the game  ###\")\n",
    "            elif self.states[h] == O:\n",
    "                print(\"  ###  O won the game  ###\")\n",
    "            else:\n",
    "                print(\"  ###  Nobody won  ###\")\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def draw_board(self):\n",
    "        print(\" -------------\")\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                v = '  .  ' if self.board[i,j]==0 else '  X  ' if self.board[i,j]==1 else '  O  '\n",
    "                print(v, end='')\n",
    "            print()\n",
    "        print(\" -------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main loop of an episode (a full game run). The loop will stop as soon as the game is over, which means that either one player managed to align 3 tokens or nobody has won.\n",
    "\n",
    "This is a turn-base game, so each agent takes turn when playing. The player 1 starts first.\n",
    "\n",
    "When an agent takes an action, the environment reaches a new state and both agents update their internal history with this state.\n",
    "\n",
    "Finally, when the game is over, each agent can update its value function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(player1, player2, env, draw=False):\n",
    "    \n",
    "    current_player = None\n",
    "    \n",
    "    env.new_game()\n",
    "    \n",
    "    while not env.game_over():\n",
    "        \n",
    "        # change the current player\n",
    "        if current_player == player1:\n",
    "            current_player = player2\n",
    "        else:\n",
    "            current_player = player1\n",
    "          \n",
    "        # draw the board\n",
    "        if draw == current_player.symbol:\n",
    "            env.draw_board()\n",
    "        \n",
    "        # current player makes a move\n",
    "        current_player.play(env)\n",
    "    \n",
    "        # update the history of each agent \n",
    "        state = compute_state_hash(env.board)\n",
    "        player1.update_state_history(state)\n",
    "        player2.update_state_history(state)\n",
    "    \n",
    "    if draw:\n",
    "        env.draw_board()\n",
    "        \n",
    "    # update the value function\n",
    "    player1.update(env)\n",
    "    player2.update(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything has been defined. We can now instantiate the game and the agents and start the learning phase. We will play 10000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n"
     ]
    }
   ],
   "source": [
    "env = Environment(BoardWidth, BoardHeight)\n",
    "player1 = Agent(X, env, 0.3, 0.1)\n",
    "player2 = Agent(O, env, 0.3, 0.1)\n",
    "\n",
    "for episode in range(10000):\n",
    "    if not episode % 200: print (episode)\n",
    "    play_game(player1, player2, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the agent, we are now defining a new interactive player. This agent does not need to learn, so only the 'play' method is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "       \n",
    "    def __init__(self, symbol):\n",
    "        self.symbol = symbol\n",
    "    \n",
    "    def play(self, env):\n",
    "        \n",
    "        # draw the board\n",
    "        env.draw_board()\n",
    "\n",
    "        while True:\n",
    "            move = input(\"Enter coordinates 'i j' for your next move (i,j=0..2): \")\n",
    "            i, j = move.split(' ')\n",
    "            i = int(i)\n",
    "            j = int(j)\n",
    "            if env.board[i,j] == _:\n",
    "                env.board[i,j] = self.symbol\n",
    "                break    \n",
    "    \n",
    "    def update(self, env):\n",
    "        pass\n",
    "        \n",
    "    def update_state_history(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play the game !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -------------\n",
      "  .    .    .  \n",
      "  .    .    .  \n",
      "  .    .    .  \n",
      " -------------\n",
      "Enter coordinates 'i j' for your next move (i,j=0..2): 0 0\n",
      "  X  0.34 0.41 \n",
      "0.41 0.50 0.33 \n",
      "0.41 0.44 0.43 \n",
      "Agent is playing greedy in (1, 1)\n",
      " -------------\n",
      "  X    .    .  \n",
      "  .    O    .  \n",
      "  .    .    .  \n",
      " -------------\n",
      "Enter coordinates 'i j' for your next move (i,j=0..2): 0 2\n",
      "  X  0.40   X  \n",
      "0.35   O  0.35 \n",
      "0.35 0.24 0.35 \n",
      "Agent is playing greedy in (0, 1)\n",
      " -------------\n",
      "  X    O    X  \n",
      "  .    O    .  \n",
      "  .    .    .  \n",
      " -------------\n",
      "Enter coordinates 'i j' for your next move (i,j=0..2): 2 1\n",
      "  X    O    X  \n",
      "0.35   O  0.32 \n",
      "0.34   X  0.32 \n",
      "Agent is playing greedy in (1, 0)\n",
      " -------------\n",
      "  X    O    X  \n",
      "  O    O    .  \n",
      "  .    X    .  \n",
      " -------------\n",
      "Enter coordinates 'i j' for your next move (i,j=0..2): 1 2\n",
      "  X    O    X  \n",
      "  O    O    X  \n",
      "0.17   X  0.12 \n",
      "Agent is playing greedy in (2, 0)\n",
      " -------------\n",
      "  X    O    X  \n",
      "  O    O    X  \n",
      "  O    X    .  \n",
      " -------------\n",
      "Enter coordinates 'i j' for your next move (i,j=0..2): 2 2\n",
      "  ###  X won the game  ###\n",
      "Play again ? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "human = HumanPlayer(X)\n",
    "\n",
    "Debug = 1\n",
    "\n",
    "stop = False\n",
    "while not stop:\n",
    "    play_game(human, player2, env)\n",
    "    stop = input(\"Play again ? [Y/n]\") == 'n'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
