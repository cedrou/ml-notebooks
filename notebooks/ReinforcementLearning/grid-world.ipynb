{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid world\n",
    "\n",
    "We define a game where a robot learns to find an optimal path in a room to a target while avoiding traps. \n",
    "\n",
    "The room is described by a 3x4 grid. Initially the robot is in (2,0), the target is in (0,3), the trap in (1,3) and the cell (1,1) is unreachable (wall or pillar).\n",
    "\n",
    "```\n",
    "    +---+---+---+---+\n",
    "    |   |   |   | +1|\n",
    "    +---+---+---+---+\n",
    "    |   |XXX|   | -1|\n",
    "    +---+---+---+---+\n",
    "    | R |   |   |   |\n",
    "    +---+---+---+---+\n",
    "    \n",
    "```\n",
    "The actions available to the robot are go North, South, East and West (N, S, E, W).\n",
    "\n",
    "This problem exposes 11 reachable states and 4 possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Grid` describes a generic room (size, rewards for each cell and actions possible from each cell) and keeps the current state (the position of the agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, height, width):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "    def configure(self, rewards, actions, transitions, start):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.start = start\n",
    "        self.restart()\n",
    "        \n",
    "    def restart(self, start=None, random=False):\n",
    "        if random: \n",
    "            K = list(self.actions.keys())\n",
    "            start = K[np.random.choice(len(K))]\n",
    "        elif start is None: \n",
    "            start = self.start\n",
    "        self.state = start\n",
    "        \n",
    "    def is_terminal_state(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        return self.is_terminal_state(self.state)\n",
    "    \n",
    "    def all_states(self):\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                yield (r,c)\n",
    "    \n",
    "    def all_actions(self):\n",
    "        return list(set(s for a in std_grid.actions.values() for s in a))\n",
    "    \n",
    "    def move(self, a):\n",
    "        if self.is_game_over(): return 0.0\n",
    "        \n",
    "        # get the transition distribution from the current state with action a\n",
    "        targets = self.transitions(self.state, a)        \n",
    "        if not targets: return 0 # move is not possible\n",
    "        \n",
    "        print (targets)\n",
    "        K = list(targets.keys())\n",
    "        V = list(targets.values())\n",
    "        self.state = K[np.random.choice(len(K), p=V)]\n",
    "        \n",
    "        return self.rewards.get(self.state, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a standard grid factory function. It creates a new `Grid` instance and configure it as described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_grid():\n",
    "    g = Grid(3,4)\n",
    "    \n",
    "    rewards = { (0,3): 1.0, \n",
    "                (1,3): -1.0\n",
    "              }\n",
    "    \n",
    "    actions = { (0,0): ['E', 'S'],\n",
    "                (0,1): ['W', 'E'],\n",
    "                (0,2): ['W', 'E', 'S'],\n",
    "                (1,0): ['N', 'S'],\n",
    "                (1,2): ['E', 'N', 'S'],\n",
    "                (2,0): ['E', 'N'],\n",
    "                (2,1): ['W', 'E'],\n",
    "                (2,2): ['W', 'E', 'N'],\n",
    "                (2,3): ['W', 'N']\n",
    "              }\n",
    "    \n",
    "    def transitions(s, a):\n",
    "        if s not in actions:return {}\n",
    "        if a not in actions[s]: return {}\n",
    "        if a=='S': return {(s[0]+1, s[1]  ): 1.0}\n",
    "        if a=='N': return {(s[0]-1, s[1]  ): 1.0}\n",
    "        if a=='E': return {(s[0]  , s[1]+1): 1.0}\n",
    "        if a=='W': return {(s[0]  , s[1]-1): 1.0}\n",
    "\n",
    "    \n",
    "    g.configure(rewards, actions, transitions, (2,0))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want to inspect the differents solutions we have found. The following functions display the value function and the policy on the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(V, g):\n",
    "    print (\"Value function\")\n",
    "    for c in range(g.width):\n",
    "        print(\"+-------\", end='')\n",
    "    print(\"+\")\n",
    "    for r in range(g.height):\n",
    "        for c in range(g.width):\n",
    "            print (f\"| {V.get((r,c),0):+0.2f} \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            print(\"+-------\", end='')\n",
    "        print(\"+\")\n",
    "        \n",
    "        \n",
    "def print_policy(pi, g):\n",
    "    print (\"Policy\")\n",
    "    \n",
    "    for c in range(g.width):\n",
    "        print(\"+-------------\", end='')\n",
    "    print(\"+\")\n",
    "    \n",
    "    for r in range(g.height):\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"|    {pi(g, 'N', s):+0.2f}    \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"| {pi(g, 'W', s):+0.2f} {pi(g, 'E', s):+0.2f} \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"|    {pi(g, 'S', s):+0.2f}    \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            print(\"+-------------\", end='')\n",
    "        print(\"+\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Problem\n",
    "Given a policy, we want to find the value function $V^\\pi(s)$.\n",
    "\n",
    "### Iterative Policy Evaluation\n",
    "\n",
    "For that we will use the **Iterative Policy Evaluation** algorithm which loops repetitively to update the value function at each state using the Bellman equation until it converges.\n",
    "\n",
    "The Bellman equation compute the value of a state from the value of the future possible states, according to a policy:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a}{\\pi(a \\mid s) \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V^\\pi(s'))}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $V^\\pi(s)$ is the value of the state $s$ according to the policy $\\pi$\n",
    "- $\\pi(a \\mid s)$ is the policy. It gives the probability of taking the action $a$ given we are in state $s$\n",
    "- $p(s',r \\mid s,a)$ is the probability of transitionning to state $s'$ and getting the reward $r$, while we are in state $s$ and take the action $a$. In our case, transitions from state to state are deterministic, so this value is always $1$ or $0$.\n",
    "- $r$ is the reward of the transition $s \\rightarrow s'$\n",
    "- $\\gamma$ is the discount factor\n",
    "\n",
    "Initially, all values are set to zero. Then we iterate continuously over all states, applying the Bellman equation to compute the value at each state. The loop finishes when all the values converge toward a stable value (which mean the difference between the new computed value and the previous one is very small).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(grid, pi, gamma, debug=False):\n",
    "    \n",
    "    # initialize the value function to 0\n",
    "    V = { s:0.0 for s in grid.all_states() }\n",
    "    \n",
    "    \n",
    "    # loop until stabilization\n",
    "    loop = 0\n",
    "    while True:\n",
    "        if debug: print (f\"--- Loop #{loop}\")\n",
    "        loop += 1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # enumerate all states\n",
    "        for s in grid.all_states():\n",
    "            if debug: print (f\"   State {s}\")\n",
    "            \n",
    "            # if terminal, the value is still zero\n",
    "            if grid.is_terminal_state(s):\n",
    "                if debug: print (f\"   -> terminal\")\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            # Bellman equation : sum over all available actions\n",
    "            v = 0\n",
    "            for a in grid.actions[s]:\n",
    "                if debug: print (f\"      Action: {a}\")\n",
    "                \n",
    "                # get the probability of taking action 'a' while in state 's'\n",
    "                pa = pi(grid, a, s)\n",
    "                if debug: print (f\"          pa={pa}\")\n",
    "                if pa == 0: continue\n",
    "                \n",
    "                # get the distribution of possible targets if we take action 'a' from state 's'\n",
    "                targets = grid.transitions(s,a)\n",
    "                if debug: print (f\"          targets={targets}\")\n",
    "                if not targets: continue\n",
    "                \n",
    "                for next_state, prob in targets.items():\n",
    "                    if debug: print (f\"              next_state={next_state}, prob={prob}\")\n",
    "                    if debug: print (f\"              V[next_state]={V[next_state]}\")\n",
    "                    v += pa * prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "\n",
    "            \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            if debug: print (f\"   Vs={V[s]} -> {v} : delta={delta}\")\n",
    "                    \n",
    "            V[s] = v\n",
    "            \n",
    "        if (delta < 1e-3):\n",
    "            break\n",
    "                \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy\n",
    "To begin, we evaluate a random policy. This policy states that we have the same probability to take any available action at every state. More formally, at each state $s$, there is a set of available action $A_s = {a_0, a_1, \\ldots , a_N }$. The probability to take the action $a_i$ is $p(a_i) = 1\\,/\\,|A_s|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.50 | +0.50 +0.50 | +0.33 +0.33 | +0.00 +0.00 |\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.33 | +0.00 +0.00 |\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.50    |\n",
      "| +0.00 +0.50 | +0.50 +0.50 | +0.33 +0.33 | +0.50 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.03 | +0.09 | +0.22 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.16 | +0.00 | -0.44 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.29 | -0.41 | -0.54 | -0.77 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def random_policy(grid, a, s):\n",
    "    if s not in grid.actions: return 0.0\n",
    "    if a not in grid.actions[s]: return 0.0\n",
    "    return 1.0 / len(grid.actions[s])\n",
    "\n",
    "\n",
    "\n",
    "std_grid = build_standard_grid()\n",
    "\n",
    "V = iterative_policy_evaluation(std_grid, random_policy, 1.0)\n",
    "\n",
    "print_policy(random_policy, std_grid)\n",
    "print_value (V, std_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed policy\n",
    "The next policy we want to evaluate is a fixed policy. At each state, the action is fixed. For our example, any state on the left column or the top row will lead to the target, and any other state will lead to the trap.\n",
    "\n",
    "```\n",
    "    +---+---+---+---+\n",
    "    | → | → | → | +1|\n",
    "    +---+---+---+---+\n",
    "    | ↑ |XXX| → | -1|\n",
    "    +---+---+---+---+\n",
    "    | ↑ | → | → | ↑ |\n",
    "    +---+---+---+---+\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| +0.81 | +0.90 | +1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.73 | +0.00 | -1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.66 | -0.81 | -0.90 | -1.00 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def fixed_policy(grid, a, s):\n",
    "    fixed = { (0,0):'E', (0,1):'E', (0,2):'E', \n",
    "              (1,0):'N',            (1,2):'E',  \n",
    "              (2,0):'N', (2,1):'E', (2,2):'E', (2,3):'N',  \n",
    "            }\n",
    "    return 1.0 if s in fixed and fixed[s] == a else 0.0\n",
    "\n",
    "\n",
    "V = iterative_policy_evaluation(std_grid, fixed_policy, 0.9)\n",
    "\n",
    "print_policy(fixed_policy, std_grid)\n",
    "print_value (V, std_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement : control problem\n",
    "\n",
    "Many RL algorithms are based on estimating value functions that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). \n",
    "\n",
    "A policy's value function assigns to each state the expected return from that state given that the agent uses the policy. The optimal value function assigns to each state the largest expected return achievable by any policy. A policy whose value function is the optimal value function is an optimal policy.\n",
    "\n",
    "With the previous algorithm, we have a way to find the value function of the game given a policy. In this part, we will see how we can find the optimal policy for this problem: this a a _control problem_.\n",
    "\n",
    "The value of a state $V^\\pi(s)$ (_state-value_ function) is the expected return starting from that state $s$. It depends on the agent’s policy:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "V^\\pi(s) & = E_\\pi[G(t) \\mid s] \\\\\n",
    "         & = \\sum_{a}{\\pi(a \\mid s) \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V^\\pi(s'))}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The value of taking the action in a state under the policy $Q^\\pi(s,a)$ (_action-value_ function) is the expected return starting from that state $s$, taking that action $a$, and thereafter following the policy $\\pi$:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V^\\pi(s'))}$$\n",
    "\n",
    "The difference is subtile but this will help us optimizing the policy. Using the current policy, we simply get $V^\\pi(s)$. To optimize this policy, we need to change some actions in this policy and see if taking this action improve the reward expectation ($V^{\\pi'}(s) > V^\\pi(s)$). As for each state we have a finite set of actions (U, D, L, R), we just go through each one until we get a better value. More formally, that is find $a \\in A$ s.t. $Q^\\pi(s,a) > Q^\\pi(s, \\pi(s))$.\n",
    "\n",
    "We are finding a new policy $\\pi'$ that gives us a bigger value than we had before: $ V^\\pi(s) \\leq V^{\\pi'}(s) $\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\pi'(s) & = \\operatorname{argmax}_a Q^\\pi(s,a) \\\\\n",
    "        & = \\operatorname{argmax}_a \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V^\\pi(s'))}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "### Policy iteration\n",
    "\n",
    "This algorithm alternates between policy evaluation (compute the value fonction under the current policy) and policy improvement (find the optimal actions to take) and continues until the policy doesn't change anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fixed_policy(state_action_map):\n",
    "    return lambda grid, a, s: 1.0 if s in state_action_map and state_action_map[s] == a else 0.0\n",
    "\n",
    "def policy_iteration(grid, gamma, debug=False):\n",
    "    # randomly initialize the actions at each state\n",
    "    V = { s:np.random.random() for s in grid.all_states() }\n",
    "    A = { s:np.random.choice([a for a in grid.all_actions()]) for s in grid.all_states() }\n",
    "    \n",
    "    # create a policy function\n",
    "    pi = build_fixed_policy(A)\n",
    "\n",
    "    loop = 0\n",
    "    while True:\n",
    "        if debug: print (f\"Loop #{loop}: A={A}\")\n",
    "        loop+=1\n",
    "        \n",
    "        # policy evaluation\n",
    "        V = iterative_policy_evaluation(grid, pi, 0.9)\n",
    "        if debug: print (f\"         V={V}\")\n",
    "        \n",
    "        # policy improvement\n",
    "        policy_changed = False\n",
    "        for s in grid.all_states():\n",
    "            if debug: print (f\"    State {s}\")\n",
    "            \n",
    "            # init max with current values\n",
    "            maxV = float('-inf')\n",
    "            maxA = ''\n",
    "                        \n",
    "            for a in grid.all_actions():\n",
    "                \n",
    "                # get p(s′,r|s,a), the distribution of possible targets if we take action 'a' from state 's'\n",
    "                targets = grid.transitions(s,a)\n",
    "                if not targets: continue\n",
    "                \n",
    "                v = 0\n",
    "                for next_state, prob in targets.items():\n",
    "                    v += prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "                \n",
    "                if debug: print (f\"      {a} -> {v}\")\n",
    "                if v >= maxV:\n",
    "                    maxV = v\n",
    "                    maxA = a\n",
    "\n",
    "            if maxA != A[s]:\n",
    "                A[s] = maxA\n",
    "                policy_changed = True\n",
    "            \n",
    "        if not policy_changed:\n",
    "            break\n",
    "            \n",
    "    # last value evaluation\n",
    "    V = iterative_policy_evaluation(grid, pi, 0.9)\n",
    "\n",
    "    return A, V\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a new grid which penalizes each move with a negative reward (for example to simulate the robot power usage). All non terminal states will have the same cost, except the (0,0) that will all twice the cost of all others to simulate an area where it is more difficult to move (sand, water, ...). The optimal policy should eventually avoid this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.20 | -0.10 | -0.10 | +1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | +0.00 | -0.10 | -1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+\n",
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 | +1.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| +0.62 | +0.80 | +1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.36 | +0.00 | +0.80 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.31 | +0.46 | +0.62 | +0.46 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def build_negative_grid(step_cost=-0.1):\n",
    "    g = build_standard_grid()\n",
    "    g.rewards.update({ s:step_cost for s in g.actions.keys() })\n",
    "    g.rewards.update({ (0,0):2*step_cost })\n",
    "    return g\n",
    "    \n",
    "neg_grid = build_negative_grid(-0.1)\n",
    "\n",
    "A,V = policy_iteration(neg_grid, 0.9)\n",
    "\n",
    "print_value (neg_grid.rewards, neg_grid)\n",
    "print_policy(build_fixed_policy(A), neg_grid)\n",
    "print_value (V, neg_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windy grid world\n",
    "So far, the transitions $p(s',r \\mid s,a)$ has been deterministic. Let's add a bit of randomness in the grid world and suppose the agent actions can be disturbed by violent gusts of wind. The probability that the expected action occurs is 0.5, but another action can also occur with a probability of 0.5/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.20 | -0.10 | -0.10 | +1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | +0.00 | -0.10 | -1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+\n",
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +0.00 | +1.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.08 | +0.16 | +0.55 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.25 | +0.00 | -0.02 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.28 | -0.28 | -0.25 | -0.41 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def build_windy_grid(step_cost=-0.1):\n",
    "    g = build_negative_grid(step_cost)\n",
    "    \n",
    "    def transitions(s, a):\n",
    "        \n",
    "        if s not in g.actions: return {}\n",
    "        if a not in g.actions[s]: return {}\n",
    "        \n",
    "        n = len(g.actions[s]) - 1\n",
    "        \n",
    "        go_s = ( min(g.height-1,s[0]+1), s[1]                )\n",
    "        go_n = ( max(0, s[0]-1)        , s[1]                )\n",
    "        go_e = ( s[0]                  , min(g.width-1,s[1]+1) )\n",
    "        go_w = ( s[0]                  , max(0, s[1]-1)      )\n",
    "        \n",
    "        if a=='S': return {go_s: 0.5 , go_n:0.5/3, go_e:0.5/3, go_w:0.5/3 }\n",
    "        if a=='N': return {go_s:0.5/3, go_n: 0.5 , go_e:0.5/3, go_w:0.5/3 }\n",
    "        if a=='E': return {go_s:0.5/3, go_n:0.5/3, go_e: 0.5 , go_w:0.5/3 }\n",
    "        if a=='W': return {go_s:0.5/3, go_n:0.5/3, go_e:0.5/3, go_w: 0.5  }\n",
    "\n",
    "    g.transitions = transitions\n",
    "    return g\n",
    "\n",
    "wind_grid = build_windy_grid()\n",
    "\n",
    "A,V = policy_iteration(wind_grid, 0.9, debug=False)\n",
    "\n",
    "print_value (wind_grid.rewards, wind_grid)\n",
    "print_policy(build_fixed_policy(A), wind_grid)\n",
    "print_value (V, wind_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, although the state (0,0) has a greater cost than the other states, the optimal policy pass through it. This is probably to avoid passing next to the trap where the wind has chances to push the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration\n",
    "\n",
    "The value iteration algorithm is a variant of the policy iteration that combines policy evaluation and policy improvement into one step:\n",
    "\n",
    "$$V^{\\pi'}(s) = \\operatorname{max}_a \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V^\\pi(s'))}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid, gamma, debug=False):\n",
    "    \n",
    "    # initialize the value function to 0\n",
    "    V = { s:0.0 for s in grid.all_states() }\n",
    "    \n",
    "    \n",
    "    # loop until stabilization\n",
    "    loop = 0\n",
    "    while True:\n",
    "        if debug: print (f\"--- Loop #{loop}\")\n",
    "        loop += 1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # enumerate all states\n",
    "        for s in grid.all_states():\n",
    "            if debug: print (f\"   State {s}\")\n",
    "            \n",
    "            # if terminal, the value is still zero\n",
    "            if grid.is_terminal_state(s):\n",
    "                if debug: print (f\"   -> terminal\")\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            maxV = float('-inf')\n",
    "            for a in grid.all_actions():\n",
    "                \n",
    "                # get p(s′,r|s,a), the distribution of possible targets if we take action 'a' from state 's'\n",
    "                targets = grid.transitions(s,a)\n",
    "                if not targets: continue\n",
    "                \n",
    "                v = 0\n",
    "                for next_state, prob in targets.items():\n",
    "                    v += prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "                \n",
    "                if debug: print (f\"      {a} -> {v}\")\n",
    "                if v >= maxV:\n",
    "                    maxV = v\n",
    "            \n",
    "            delta = max(delta, abs(maxV - V[s]))\n",
    "            if debug: print (f\"   Vs={V[s]} -> {maxV} : delta={delta}\")\n",
    "                    \n",
    "            V[s] = maxV\n",
    "            \n",
    "        if (delta < 1e-3):\n",
    "            break\n",
    "    \n",
    "    for s in grid.all_states():\n",
    "        if debug: print (f\"    State {s}\")\n",
    "\n",
    "        maxV = float('-inf')\n",
    "        maxA = ''\n",
    "\n",
    "        for a in grid.all_actions():\n",
    "\n",
    "            # get p(s′,r|s,a), the distribution of possible targets if we take action 'a' from state 's'\n",
    "            targets = grid.transitions(s,a)\n",
    "            if not targets: continue\n",
    "\n",
    "            v = 0\n",
    "            for next_state, prob in targets.items():\n",
    "                v += prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "\n",
    "            if debug: print (f\"      {a} -> {v}\")\n",
    "            if v >= maxV:\n",
    "                maxV = v\n",
    "                maxA = a\n",
    "\n",
    "        A[s] = maxA\n",
    "\n",
    "    return A, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.20 | -0.10 | -0.10 | +1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | +0.00 | -0.10 | -1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+\n",
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 | +1.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| +0.62 | +0.80 | +1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.36 | +0.00 | +0.80 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.31 | +0.46 | +0.62 | +0.46 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "A,V = value_iteration(neg_grid, 0.9, debug=False)\n",
    "\n",
    "print_value (neg_grid.rewards, neg_grid)\n",
    "print_policy(build_fixed_policy(A), neg_grid)\n",
    "print_value (V, neg_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value iteration algorithm should be more performant than the policy iteration algorithm. And indeed, we observe a factor 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.95 ms ± 429 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "389 µs ± 39.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit policy_iteration(neg_grid, 0.9, debug=False)\n",
    "%timeit value_iteration(neg_grid, 0.9, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo method\n",
    "\n",
    "Dynamic Programming supposes that we have access to the full model of the environment; oncluding the transition rules $p(s',r|s,a)$. But, most of the time, this is not the case et we introduce here the Monte-Carlo algorithm which is designed to learn purely from experience.\n",
    "\n",
    "MC methods are a broad class of algorithms that rely on repeated random sampling to obtain numerical results.\n",
    "\n",
    "in MC, instead of computing the _true_ expected value of $G$ at each state $s$, we calculate a _sample mean_ from a number of episodes $N$:\n",
    "\n",
    "$$\\bar V^\\pi(s) = \\frac{1}{N}\\sum_{i=1}^{N}G_{i,s}$$\n",
    "\n",
    "$G$ is generated by playing $N$ episodes, log the states and reward sequences, and use the recursive definition of $G(t)$:\n",
    "\n",
    "$$G(t) = r(t+1) + \\gamma G(t+1)$$\n",
    "\n",
    "Then, once we have $\\{s,G\\}$ pairs, we average them for each state.\n",
    "\n",
    "In the case where we see the same state more than once in an episode, we have 2 choices: either we use only the first and discard the others (this is called \"first-visit MC\"), or we use all values as different samples (\"every-visit MC\"). Surprisingly, both methods lead to the same results, so we will use only the first-visit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc_predition(grid, pi, N, gamma):\n",
    "\n",
    "    states_and_returns = {} # key:state - value:array of returns\n",
    "    V = {}\n",
    "    \n",
    "    for i in range(N):\n",
    "        snr = play_episode(grid, pi, gamma)\n",
    "        seen_states = set()\n",
    "        for (s,g) in snr:\n",
    "            if s not in seen_states:\n",
    "                seen_states.add(s)\n",
    "                if s in states_and_returns:\n",
    "                    states_and_returns[s].append(g)\n",
    "                else:\n",
    "                    states_and_returns[s] = [g]\n",
    "        \n",
    "        for s, returns in states_and_returns.items():\n",
    "            V[s] = np.mean(returns)\n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an action to do from state s according to the policy pi\n",
    "def select_action(grid, pi, s):\n",
    "    A = grid.actions[s]\n",
    "    p = [pi(grid, a, s) for a in A]\n",
    "    return np.random.choice(A, p=p)\n",
    "    \n",
    "\n",
    "def play_episode(grid, pi, gamma):\n",
    "    grid.restart(random=True)\n",
    "    \n",
    "    # play until gme is over\n",
    "    s = grid.state \n",
    "    states_and_rewards = [(s,0)]\n",
    "    while not grid.is_game_over():    \n",
    "        a = select_action(grid, pi, s)\n",
    "        print(s,\"->\", a)\n",
    "        r = grid.move(a)\n",
    "        s = grid.state\n",
    "        states_and_rewards.append((s,r))\n",
    "        \n",
    "    # iteratively compute returns from rewards\n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    for s,r in reversed(states_and_rewards):\n",
    "        states_and_returns.append((s,G))\n",
    "        G = r + gamma * G\n",
    "\n",
    "    states_and_returns.reverse()\n",
    "\n",
    "    return states_and_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first test this with the fixed policy, and we should retrieve the same value function as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| +0.81 | +0.90 | +1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.73 | +0.00 | -1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.66 | -0.81 | -0.90 | -1.00 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "V = first_visit_mc_predition(std_grid, fixed_policy, 100, 0.9)\n",
    "\n",
    "print_policy(fixed_policy, std_grid)\n",
    "print_value (V, std_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This standard grid is fully deterministic as the target state is completely determined by the current state and the action taken. In this case, MC is not really needed and does not give better results than DP. We will now use the windy grid that adds a bit of randomness in the target state computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) -> E\n",
      "{(2, 1): 0.16666666666666666, (1, 1): 0.16666666666666666, (2, 2): 0.5, (2, 0): 0.16666666666666666}\n",
      "(2, 2) -> E\n",
      "{(2, 2): 0.16666666666666666, (1, 2): 0.16666666666666666, (2, 3): 0.5, (2, 1): 0.16666666666666666}\n",
      "(2, 1) -> E\n",
      "{(2, 1): 0.16666666666666666, (1, 1): 0.16666666666666666, (2, 2): 0.5, (2, 0): 0.16666666666666666}\n",
      "(2, 0) -> N\n",
      "{(2, 0): 0.16666666666666666, (1, 0): 0.5, (2, 1): 0.16666666666666666}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-48822f63afaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_visit_mc_predition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwind_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfixed_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwind_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint_value\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwind_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-410f6cf20685>\u001b[0m in \u001b[0;36mfirst_visit_mc_predition\u001b[1;34m(grid, pi, N, gamma)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0msnr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mseen_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msnr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-dc35a327b60f>\u001b[0m in \u001b[0;36mplay_episode\u001b[1;34m(grid, pi, gamma)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"->\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mstates_and_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-6ae617bb52c6>\u001b[0m in \u001b[0;36mmove\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "V = first_visit_mc_predition(wind_grid, fixed_policy, 100, 0.9)\n",
    "\n",
    "print_policy(fixed_policy, wind_grid)\n",
    "print_value (V, wind_grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
