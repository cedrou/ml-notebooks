{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid world\n",
    "\n",
    "In this notebook, we will demonstrate some algorithms used in **Dynamic Programming**.\n",
    "\n",
    "We define a game where a robot learns to find an optimal path in a room to a target while avoiding traps. \n",
    "\n",
    "The room is described by a 3x4 grid. Initially the robot is in (2,0), the target is in (0,3), the trap in (1,3) and the cell (1,1) is unreachable (wall or pillar).\n",
    "\n",
    "```\n",
    "    +---+---+---+---+\n",
    "    |   |   |   | +1|\n",
    "    +---+---+---+---+\n",
    "    |   |XXX|   | -1|\n",
    "    +---+---+---+---+\n",
    "    | R |   |   |   |\n",
    "    +---+---+---+---+\n",
    "    \n",
    "```\n",
    "The actions available to the robot are up, down, left and right (U,D,L,R).\n",
    "\n",
    "This problem exposes 11 states and 4 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DEBUG=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Grid` describes a generic room (size, rewards for each cell and actions possible from each cell) and keeps the current state (the position of the agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, height, width):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "    def configure(self, rewards, actions, transitions, start):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.state = start\n",
    "        \n",
    "    def is_terminal_state(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        return is_terminal_state(self.state)\n",
    "    \n",
    "    def all_states(self):\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                yield (r,c)\n",
    "    \n",
    "    def move(self, a):\n",
    "        if self.is_game_over(): return 0.0\n",
    "        \n",
    "        # get the transition distribution from the current state with action a\n",
    "        targets = self.transitions(self.state, a)        \n",
    "        if not targets: return 0 # move is not possible\n",
    "        \n",
    "        self.state = np.random.choice(targets.keys(), p=targets.values())\n",
    "        \n",
    "        return self.reward.get(self.state, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define 2 grid factory functions: `build_standard_grid` and `build_negative_grid`. Both are creating a new `Grid` instance and configure it as described above. The difference is that the second one will penalize each move with a negative award, while the first one does not give any award on empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_grid():\n",
    "    g = Grid(3,4)\n",
    "    \n",
    "    rewards = { (0,3): 1.0, \n",
    "                (1,3): -1.0\n",
    "              }\n",
    "    \n",
    "    actions = { (0,0): ['R', 'D'],\n",
    "                (0,1): ['L', 'R'],\n",
    "                (0,2): ['L', 'R', 'D'],\n",
    "                (1,0): ['U', 'D'],\n",
    "                (1,2): ['R', 'U', 'D'],\n",
    "                (2,0): ['R', 'U'],\n",
    "                (2,1): ['L', 'R'],\n",
    "                (2,2): ['L', 'R', 'U'],\n",
    "                (2,3): ['L', 'U']\n",
    "              }\n",
    "    \n",
    "    def transitions(s, a):\n",
    "        if a not in actions[s]: return {}\n",
    "        if a=='D': return {(s[0]+1, s[1]  ): 1.0}\n",
    "        if a=='U': return {(s[0]-1, s[1]  ): 1.0}\n",
    "        if a=='R': return {(s[0]  , s[1]+1): 1.0}\n",
    "        if a=='L': return {(s[0]  , s[1]-1): 1.0}\n",
    "\n",
    "    \n",
    "    g.configure(rewards, actions, transitions, (2,0))\n",
    "    return g\n",
    "\n",
    "def build_negative_grid(step_cost=-0.1):\n",
    "    g = build_standard_grid()\n",
    "    g.reward.update({ s:step_cost for s in policy.keys() })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want to inspect the differents solutions we have found. The following functions display the value function and the policy on the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(V, g):\n",
    "    print (\"Value function\")\n",
    "    for c in range(g.width):\n",
    "        print(\"+-------\", end='')\n",
    "    print(\"+\")\n",
    "    for r in range(g.height):\n",
    "        for c in range(g.width):\n",
    "            print (f\"| {V.get((r,c),0):+0.2f} \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            print(\"+-------\", end='')\n",
    "        print(\"+\")\n",
    "        \n",
    "        \n",
    "def print_policy(pi, g):\n",
    "    print (\"Policy\")\n",
    "    \n",
    "    for c in range(g.width):\n",
    "        print(\"+-------------\", end='')\n",
    "    print(\"+\")\n",
    "    \n",
    "    for r in range(g.height):\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"|    {pi(g, 'U', s):+0.2f}    \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"| {pi(g, 'L', s):+0.2f} {pi(g, 'R', s):+0.2f} \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"|    {pi(g, 'D', s):+0.2f}    \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            print(\"+-------------\", end='')\n",
    "        print(\"+\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation\n",
    "\n",
    "In the next steps, we will need to evaluate different policies. For that we will use the **Iterative Policy Evaluation** algorithm which loops repetitively to update the value function at each state using the Bellman equation until it converges.\n",
    "\n",
    "The Bellman equation compute the value of a state from the value of the future possible states, according to a policy:\n",
    "\n",
    "$$V_\\pi(s) = \\sum_{a}{\\pi(a \\mid s) \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V_\\pi(s'))}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $V_\\pi(s)$ is the value of the state $s$ according to the policy $\\pi$\n",
    "- $\\pi(a \\mid s)$ is the policy. It gives the probability of taking the action $a$ given we are in state $s$\n",
    "- $p(s',r \\mid s,a)$ is the probability of transitionning to state $s'$ and getting the reward $r$, while we are in state $s$ and take the action $a$. In our case, transitions from state to state are deterministic, so this value is always $1$ or $0$.\n",
    "- $r$ is the reward of the transition $s \\rightarrow s'$\n",
    "- $\\gamma$ is the discount factor\n",
    "\n",
    "Initially, all values are set to zero. Then we iterate continuously over all states, applying the Bellman equation to compute the value at each state. The loop finishes when all the values converge toward a stable value (which mean the difference between the new computed value and the previous one is very small).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(grid, pi, gamma):\n",
    "    \n",
    "    # initialize the value function to 0\n",
    "    V = { s:0.0 for s in grid.all_states() }\n",
    "    \n",
    "    \n",
    "    # loop until stabilization\n",
    "    loop = 0\n",
    "    while True:\n",
    "        if DEBUG: print (f\"--- Loop #{loop}\")\n",
    "        delta = 0\n",
    "        \n",
    "        # enumerate all states\n",
    "        for s in grid.all_states():\n",
    "            if DEBUG: print (f\"   State {s}\")\n",
    "            \n",
    "            # if terminal, the value is still zero\n",
    "            if grid.is_terminal_state(s):\n",
    "                if DEBUG: print (f\"   -> terminal\")\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            # Bellman equation : sum over all available actions\n",
    "            v = 0\n",
    "            for a in grid.actions[s]:\n",
    "                if DEBUG: print (f\"      Action: {a}\")\n",
    "                \n",
    "                # get the probability of taking action 'a' while in state 's'\n",
    "                pa = pi(grid, a, s)\n",
    "                if DEBUG: print (f\"          pa={pa}\")\n",
    "                if pa == 0: continue\n",
    "                \n",
    "                # get the distribution of possible targets if we take action 'a' from state 's'\n",
    "                targets = grid.transitions(s,a)\n",
    "                if DEBUG: print (f\"          targets={targets}\")\n",
    "                if not targets: continue\n",
    "                \n",
    "                for next_state, prob in targets.items():\n",
    "                    if DEBUG: print (f\"              next_state={next_state}, prob={prob}\")\n",
    "                    if DEBUG: print (f\"              V[next_state]={V[next_state]}\")\n",
    "                    v += pa * prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "\n",
    "            \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            if DEBUG: print (f\"   Vs={V[s]} -> {v} : delta={delta}\")\n",
    "                    \n",
    "            V[s] = v\n",
    "            \n",
    "        if (delta < 1e-3):\n",
    "            break\n",
    "                \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy\n",
    "To begin, we evaluate a random policy. This policy states that we have the same probability to take any available action at every state. More formally, at each state $s$, there is a set of available action $A_s = {a_0, a_1, \\ldots , a_N }$. The probability to take the action $a_i$ is $p(a_i) = 1\\,/\\,|A_s|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.50 | +0.50 +0.50 | +0.33 +0.33 | +0.00 +0.00 |\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.33 | +0.00 +0.00 |\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.50    |\n",
      "| +0.00 +0.50 | +0.50 +0.50 | +0.33 +0.33 | +0.50 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.03 | +0.09 | +0.22 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.16 | +0.00 | -0.44 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.29 | -0.41 | -0.54 | -0.77 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def random_policy(grid, a, s):\n",
    "    if s not in grid.actions: return 0.0\n",
    "    if a not in grid.actions[s]: return 0.0\n",
    "    return 1.0 / len(grid.actions[s])\n",
    "\n",
    "\n",
    "\n",
    "std_grid = build_standard_grid()\n",
    "\n",
    "V = iterative_policy_evaluation(std_grid, random_policy, 1.0)\n",
    "\n",
    "print_policy(random_policy, std_grid)\n",
    "print_value (V, std_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed policy\n",
    "The next policy we want to evaluate is a fixed policy. At each state, the action is fixed. For our example, any state on the left column or the top row will lead to the target, and any other state will lead to the trap.\n",
    "\n",
    "```\n",
    "    +---+---+---+---+\n",
    "    | → | → | → | +1|\n",
    "    +---+---+---+---+\n",
    "    | ↑ |XXX| → | -1|\n",
    "    +---+---+---+---+\n",
    "    | ↑ | → | → | ↑ |\n",
    "    +---+---+---+---+\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| +0.81 | +0.90 | +1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.73 | +0.00 | -1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.66 | -0.81 | -0.90 | -1.00 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def fixed_policy(grid, a, s):\n",
    "    fixed = { (0,0):'R', (0,1):'R', (0,2):'R', \n",
    "              (1,0):'U',            (1,2):'R',  \n",
    "              (2,0):'U', (2,1):'R', (2,2):'R', (2,3):'U',  \n",
    "            }\n",
    "    return 1.0 if s in fixed and fixed[s] == a else 0.0\n",
    "\n",
    "\n",
    "V = iterative_policy_evaluation(std_grid, fixed_policy, 0.9)\n",
    "\n",
    "print_policy(fixed_policy, std_grid)\n",
    "print_value (V, std_grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
