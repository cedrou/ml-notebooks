{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid world\n",
    "\n",
    "In this notebook, we will demonstrate some algorithms used in **Dynamic Programming**.\n",
    "\n",
    "We define a game where a robot learns to find an optimal path in a room to a target while avoiding traps. \n",
    "\n",
    "The room is described by a 3x4 grid. Initially the robot is in (2,0), the target is in (0,3), the trap in (1,3) and the cell (1,1) is unreachable (wall or pillar).\n",
    "\n",
    "```\n",
    "    +---+---+---+---+\n",
    "    |   |   |   | +1|\n",
    "    +---+---+---+---+\n",
    "    |   |XXX|   | -1|\n",
    "    +---+---+---+---+\n",
    "    | R |   |   |   |\n",
    "    +---+---+---+---+\n",
    "    \n",
    "```\n",
    "The actions available to the robot are go North, South, East and West (N, S, E, W).\n",
    "\n",
    "This problem exposes 11 reachable states and 4 possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Grid` describes a generic room (size, rewards for each cell and actions possible from each cell) and keeps the current state (the position of the agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, height, width):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "    def configure(self, rewards, actions, transitions, start):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.state = start\n",
    "        \n",
    "    def is_terminal_state(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        return is_terminal_state(self.state)\n",
    "    \n",
    "    def all_states(self):\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                yield (r,c)\n",
    "    \n",
    "    def all_actions(self):\n",
    "        return set(s for a in std_grid.actions.values() for s in a)\n",
    "    \n",
    "    def move(self, a):\n",
    "        if self.is_game_over(): return 0.0\n",
    "        \n",
    "        # get the transition distribution from the current state with action a\n",
    "        targets = self.transitions(self.state, a)        \n",
    "        if not targets: return 0 # move is not possible\n",
    "        \n",
    "        self.state = np.random.choice(targets.keys(), p=targets.values())\n",
    "        \n",
    "        return self.reward.get(self.state, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a standard grid factory function. It creates a new `Grid` instance and configure it as described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_grid():\n",
    "    g = Grid(3,4)\n",
    "    \n",
    "    rewards = { (0,3): 1.0, \n",
    "                (1,3): -1.0\n",
    "              }\n",
    "    \n",
    "    actions = { (0,0): ['E', 'S'],\n",
    "                (0,1): ['W', 'E'],\n",
    "                (0,2): ['W', 'E', 'S'],\n",
    "                (1,0): ['N', 'S'],\n",
    "                (1,2): ['E', 'N', 'S'],\n",
    "                (2,0): ['E', 'N'],\n",
    "                (2,1): ['W', 'E'],\n",
    "                (2,2): ['W', 'E', 'N'],\n",
    "                (2,3): ['W', 'N']\n",
    "              }\n",
    "    \n",
    "    def transitions(s, a):\n",
    "        if s not in actions:return {}\n",
    "        if a not in actions[s]: return {}\n",
    "        if a=='S': return {(s[0]+1, s[1]  ): 1.0}\n",
    "        if a=='N': return {(s[0]-1, s[1]  ): 1.0}\n",
    "        if a=='E': return {(s[0]  , s[1]+1): 1.0}\n",
    "        if a=='W': return {(s[0]  , s[1]-1): 1.0}\n",
    "\n",
    "    \n",
    "    g.configure(rewards, actions, transitions, (2,0))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want to inspect the differents solutions we have found. The following functions display the value function and the policy on the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(V, g):\n",
    "    print (\"Value function\")\n",
    "    for c in range(g.width):\n",
    "        print(\"+-------\", end='')\n",
    "    print(\"+\")\n",
    "    for r in range(g.height):\n",
    "        for c in range(g.width):\n",
    "            print (f\"| {V.get((r,c),0):+0.2f} \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            print(\"+-------\", end='')\n",
    "        print(\"+\")\n",
    "        \n",
    "        \n",
    "def print_policy(pi, g):\n",
    "    print (\"Policy\")\n",
    "    \n",
    "    for c in range(g.width):\n",
    "        print(\"+-------------\", end='')\n",
    "    print(\"+\")\n",
    "    \n",
    "    for r in range(g.height):\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"|    {pi(g, 'N', s):+0.2f}    \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"| {pi(g, 'W', s):+0.2f} {pi(g, 'E', s):+0.2f} \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"|    {pi(g, 'S', s):+0.2f}    \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            print(\"+-------------\", end='')\n",
    "        print(\"+\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Problem\n",
    "Given a policy, we want to find the value function $V^\\pi(s)$.\n",
    "\n",
    "### Iterative Policy Evaluation\n",
    "\n",
    "For that we will use the **Iterative Policy Evaluation** algorithm which loops repetitively to update the value function at each state using the Bellman equation until it converges.\n",
    "\n",
    "The Bellman equation compute the value of a state from the value of the future possible states, according to a policy:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a}{\\pi(a \\mid s) \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V^\\pi(s'))}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $V^\\pi(s)$ is the value of the state $s$ according to the policy $\\pi$\n",
    "- $\\pi(a \\mid s)$ is the policy. It gives the probability of taking the action $a$ given we are in state $s$\n",
    "- $p(s',r \\mid s,a)$ is the probability of transitionning to state $s'$ and getting the reward $r$, while we are in state $s$ and take the action $a$. In our case, transitions from state to state are deterministic, so this value is always $1$ or $0$.\n",
    "- $r$ is the reward of the transition $s \\rightarrow s'$\n",
    "- $\\gamma$ is the discount factor\n",
    "\n",
    "Initially, all values are set to zero. Then we iterate continuously over all states, applying the Bellman equation to compute the value at each state. The loop finishes when all the values converge toward a stable value (which mean the difference between the new computed value and the previous one is very small).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(grid, pi, gamma, debug=False):\n",
    "    \n",
    "    # initialize the value function to 0\n",
    "    V = { s:0.0 for s in grid.all_states() }\n",
    "    \n",
    "    \n",
    "    # loop until stabilization\n",
    "    loop = 0\n",
    "    while True:\n",
    "        if debug: print (f\"--- Loop #{loop}\")\n",
    "        loop += 1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # enumerate all states\n",
    "        for s in grid.all_states():\n",
    "            if debug: print (f\"   State {s}\")\n",
    "            \n",
    "            # if terminal, the value is still zero\n",
    "            if grid.is_terminal_state(s):\n",
    "                if debug: print (f\"   -> terminal\")\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            # Bellman equation : sum over all available actions\n",
    "            v = 0\n",
    "            for a in grid.actions[s]:\n",
    "                if debug: print (f\"      Action: {a}\")\n",
    "                \n",
    "                # get the probability of taking action 'a' while in state 's'\n",
    "                pa = pi(grid, a, s)\n",
    "                if debug: print (f\"          pa={pa}\")\n",
    "                if pa == 0: continue\n",
    "                \n",
    "                # get the distribution of possible targets if we take action 'a' from state 's'\n",
    "                targets = grid.transitions(s,a)\n",
    "                if debug: print (f\"          targets={targets}\")\n",
    "                if not targets: continue\n",
    "                \n",
    "                for next_state, prob in targets.items():\n",
    "                    if debug: print (f\"              next_state={next_state}, prob={prob}\")\n",
    "                    if debug: print (f\"              V[next_state]={V[next_state]}\")\n",
    "                    v += pa * prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "\n",
    "            \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            if debug: print (f\"   Vs={V[s]} -> {v} : delta={delta}\")\n",
    "                    \n",
    "            V[s] = v\n",
    "            \n",
    "        if (delta < 1e-3):\n",
    "            break\n",
    "                \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy\n",
    "To begin, we evaluate a random policy. This policy states that we have the same probability to take any available action at every state. More formally, at each state $s$, there is a set of available action $A_s = {a_0, a_1, \\ldots , a_N }$. The probability to take the action $a_i$ is $p(a_i) = 1\\,/\\,|A_s|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.50 | +0.50 +0.50 | +0.33 +0.33 | +0.00 +0.00 |\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.33 | +0.00 +0.00 |\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.50    |    +0.00    |    +0.33    |    +0.50    |\n",
      "| +0.00 +0.50 | +0.50 +0.50 | +0.33 +0.33 | +0.50 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.03 | +0.09 | +0.22 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.16 | +0.00 | -0.44 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.29 | -0.41 | -0.54 | -0.77 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def random_policy(grid, a, s):\n",
    "    if s not in grid.actions: return 0.0\n",
    "    if a not in grid.actions[s]: return 0.0\n",
    "    return 1.0 / len(grid.actions[s])\n",
    "\n",
    "\n",
    "\n",
    "std_grid = build_standard_grid()\n",
    "\n",
    "V = iterative_policy_evaluation(std_grid, random_policy, 1.0)\n",
    "\n",
    "print_policy(random_policy, std_grid)\n",
    "print_value (V, std_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed policy\n",
    "The next policy we want to evaluate is a fixed policy. At each state, the action is fixed. For our example, any state on the left column or the top row will lead to the target, and any other state will lead to the trap.\n",
    "\n",
    "```\n",
    "    +---+---+---+---+\n",
    "    | → | → | → | +1|\n",
    "    +---+---+---+---+\n",
    "    | ↑ |XXX| → | -1|\n",
    "    +---+---+---+---+\n",
    "    | ↑ | → | → | ↑ |\n",
    "    +---+---+---+---+\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| +0.81 | +0.90 | +1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.73 | +0.00 | -1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.66 | -0.81 | -0.90 | -1.00 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def fixed_policy(grid, a, s):\n",
    "    fixed = { (0,0):'E', (0,1):'E', (0,2):'E', \n",
    "              (1,0):'N',            (1,2):'E',  \n",
    "              (2,0):'N', (2,1):'E', (2,2):'E', (2,3):'N',  \n",
    "            }\n",
    "    return 1.0 if s in fixed and fixed[s] == a else 0.0\n",
    "\n",
    "\n",
    "V = iterative_policy_evaluation(std_grid, fixed_policy, 0.9)\n",
    "\n",
    "print_policy(fixed_policy, std_grid)\n",
    "print_value (V, std_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement : control problem\n",
    "\n",
    "Many RL algorithms are based on estimating value functions that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). \n",
    "\n",
    "A policy's value function assigns to each state the expected return from that state given that the agent uses the policy. The optimal value function assigns to each state the largest expected return achievable by any policy. A policy whose value function is the optimal value function is an optimal policy.\n",
    "\n",
    "With the previous algorithm, we have a way to find the value function of the game given a policy. In this part, we will see how we can find the optimal policy for this problem: this a a _control problem_.\n",
    "\n",
    "The value of a state $V^\\pi(s)$ (_state-value_ function) is the expected return starting from that state $s$. It depends on the agent’s policy:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "V^\\pi(s) & = E_\\pi[G(t) \\mid s] \\\\\n",
    "         & = \\sum_{a}{\\pi(a \\mid s) \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V^\\pi(s'))}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The value of taking the action in a state under the policy $Q^\\pi(s,a)$ (_action-value_ function) is the expected return starting from that state $s$, taking that action $a$, and thereafter following the policy $\\pi$:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V^\\pi(s'))}$$\n",
    "\n",
    "The difference is subtile but this will help us optimizing the policy. Using the current policy, we simply get $V^\\pi(s)$. To optimize this policy, we need to change some actions in this policy and see if taking this action improve the reward expectation ($V^{\\pi'}(s) > V^\\pi(s)$). As for each state we have a finite set of actions (U, D, L, R), we just go through each one until we get a better value. More formally, that is find $a \\in A$ s.t. $Q^\\pi(s,a) > Q^\\pi(s, \\pi(s))$.\n",
    "\n",
    "We are finding a new policy $\\pi'$ that gives us a bigger value than we had before: $ V^\\pi(s) \\leq V^{\\pi'}(s) $\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\pi'(s) & = \\operatorname{argmax}_a Q^\\pi(s,a) \\\\\n",
    "        & = \\operatorname{argmax}_a \\sum_{s'}\\sum_{r}{p(s',r \\mid s,a)(r + \\gamma V^\\pi(s'))}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "### Policy iteration\n",
    "\n",
    "This algorithm alternates between policy evaluation (compute the value fonction under the current policy) and policy improvement (find the optimal actions to take) and continues until the policy doesn't change anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fixed_policy(state_action_map):\n",
    "    return lambda grid, a, s: 1.0 if s in state_action_map and state_action_map[s] == a else 0.0\n",
    "\n",
    "def policy_iteration(grid, gamma, debug=False):\n",
    "    # randomly initialize the actions at each state\n",
    "    V = { s:np.random.random() for s in grid.all_states() }\n",
    "    A = { s:np.random.choice([a for a in grid.all_actions()]) for s in grid.all_states() }\n",
    "    \n",
    "    # create a policy function\n",
    "    pi = build_fixed_policy(A)\n",
    "\n",
    "    loop = 0\n",
    "    while True:\n",
    "        if debug: print (f\"Loop #{loop}: A={A}\")\n",
    "        loop+=1\n",
    "        \n",
    "        # policy evaluation\n",
    "        V = iterative_policy_evaluation(grid, pi, 0.9)\n",
    "        if debug: print (f\"         V={V}\")\n",
    "        \n",
    "        # policy improvement\n",
    "        policy_changed = False\n",
    "        for s in grid.all_states():\n",
    "            if debug: print (f\"    State {s}\")\n",
    "            \n",
    "            # init max with current values\n",
    "            maxV = float('-inf')\n",
    "            maxA = ''\n",
    "                        \n",
    "            for a in grid.all_actions():\n",
    "                \n",
    "                # get p(s′,r|s,a), the distribution of possible targets if we take action 'a' from state 's'\n",
    "                targets = grid.transitions(s,a)\n",
    "                if not targets: continue\n",
    "                \n",
    "                v = 0\n",
    "                for next_state, prob in targets.items():\n",
    "                    v += prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "                \n",
    "                if debug: print (f\"      {a} -> {v}\")\n",
    "                if v >= maxV:\n",
    "                    maxV = v\n",
    "                    maxA = a\n",
    "\n",
    "            if maxA != A[s]:\n",
    "                A[s] = maxA\n",
    "                policy_changed = True\n",
    "            \n",
    "        if not policy_changed:\n",
    "            break\n",
    "            \n",
    "    # last value evaluation\n",
    "    V = iterative_policy_evaluation(grid, pi, 0.9)\n",
    "\n",
    "    return A, V\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a new grid which penalizes each move with a negative reward (for example to simulate the robot power usage). All non terminal states will have the same cost, except the (0,0) that will all twice the cost of all others to simulate an area where it is more difficult to move (sand, water, ...). The optimal policy should eventually avoid this state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.20 | -0.10 | -0.10 | +1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | +0.00 | -0.10 | -1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+\n",
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 | +1.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| +0.62 | +0.80 | +1.00 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.36 | +0.00 | +0.80 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| +0.31 | +0.46 | +0.62 | +0.46 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def build_negative_grid(step_cost=-0.1):\n",
    "    g = build_standard_grid()\n",
    "    g.rewards.update({ s:step_cost for s in g.actions.keys() })\n",
    "    g.rewards.update({ (0,0):2*step_cost })\n",
    "    return g\n",
    "    \n",
    "neg_grid = build_negative_grid(-0.1)\n",
    "\n",
    "A,V = policy_iteration(neg_grid, 0.9)\n",
    "\n",
    "print_value (neg_grid.rewards, neg_grid)\n",
    "print_policy(build_fixed_policy(A), neg_grid)\n",
    "print_value (V, neg_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windy grid world\n",
    "So far, the transitions $p(s',r \\mid s,a)$ has been deterministic. Let's add a bit of randomness in the grid world and suppose the agent actions can be disturbed by violent gusts of wind. The probability that the expected action occurs is 0.5, but another action can also occur with a probability of 0.5/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.20 | -0.10 | -0.10 | +1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | +0.00 | -0.10 | -1.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+\n",
      "Policy\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +1.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +0.00 | +1.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+\n",
      "| -0.08 | +0.16 | +0.55 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.25 | +0.00 | -0.02 | +0.00 |\n",
      "+-------+-------+-------+-------+\n",
      "| -0.28 | -0.28 | -0.25 | -0.41 |\n",
      "+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def build_windy_grid(step_cost=-0.1):\n",
    "    g = build_negative_grid(step_cost)\n",
    "    \n",
    "    def transitions(s, a):\n",
    "        \n",
    "        if s not in g.actions: return {}\n",
    "        if a not in g.actions[s]: return {}\n",
    "        \n",
    "        go_s = ( min(g.height-1,s[0]+1), s[1]                )\n",
    "        go_n = ( max(0, s[0]-1)        , s[1]                )\n",
    "        go_e = ( s[0]                  , min(g.width-1,s[1]+1) )\n",
    "        go_w = ( s[0]                  , max(0, s[1]-1)      )\n",
    "        \n",
    "        if a=='S': return {go_s: 0.5 , go_n:0.5/3, go_e:0.5/3, go_w:0.5/3 }\n",
    "        if a=='N': return {go_s:0.5/3, go_n: 0.5 , go_e:0.5/3, go_w:0.5/3 }\n",
    "        if a=='E': return {go_s:0.5/3, go_n:0.5/3, go_e: 0.5 , go_w:0.5/3 }\n",
    "        if a=='W': return {go_s:0.5/3, go_n:0.5/3, go_e:0.5/3, go_w: 0.5  }\n",
    "\n",
    "    g.transitions = transitions\n",
    "    return g\n",
    "\n",
    "wind_grid = build_windy_grid()\n",
    "\n",
    "A,V = policy_iteration(wind_grid, 0.9, debug=False)\n",
    "\n",
    "print_value (wind_grid.rewards, wind_grid)\n",
    "print_policy(build_fixed_policy(A), wind_grid)\n",
    "print_value (V, wind_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, although the state (0,0) has a greater cost than the other states, the optimal policy pass through it. This is probably to avoid passing next to the trap where the wind has chances to push the agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
